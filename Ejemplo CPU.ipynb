{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ejemplo CPU.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1F0ymH62qLxdXBTUhewbgf24gNLplvGPt","authorship_tag":"ABX9TyNc9aIdiR8qdR/hWkaGpQDJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ibUOWCWtevny"},"source":["# Implementación en arquitectura **CPU**"]},{"cell_type":"code","metadata":{"id":"xt4KJEHVRip7","executionInfo":{"status":"ok","timestamp":1615045999937,"user_tz":300,"elapsed":471,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}}},"source":["import numpy as np\r\n","import os\r\n","import gzip\r\n","\r\n","## Función para cargar el set de datos de validación junto con la categoria \r\n","## a la que pertenece cada una de las imagenes, es decir, el homologo en modelos a la variable\r\n","## respuesta y.\r\n","\r\n","def load_mnist(ruta, tipo='train'):\r\n","\r\n","    ruta_categorias = os.path.join(ruta, '%s-labels-idx1-ubyte.gz' % tipo)\r\n","    ruta_imagenes = os.path.join(ruta, '%s-images-idx3-ubyte.gz' % tipo)\r\n","    \r\n","    with gzip.open(ruta_categorias, 'rb') as rut_cat:\r\n","        etiquetas = np.frombuffer(rut_cat.read(), dtype=np.uint8, offset=8)\r\n","\r\n","    with gzip.open(ruta_imagenes, 'rb') as rut_imgs:\r\n","        imagenes = np.frombuffer(rut_imgs.read(), dtype=np.uint8, offset=16).reshape(len(etiquetas), 784)\r\n","\r\n","    return imagenes, etiquetas\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Q3lXlh_TEn7","executionInfo":{"status":"ok","timestamp":1615046029054,"user_tz":300,"elapsed":23492,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}},"outputId":"dfc7060b-cda0-46f8-c3fe-ca15e0e471df"},"source":["### Carga de datos\r\n","\r\n","## Con este código se carga la libreria Drive de google colab, que permite realizar la lectura de datos\r\n","## desde google drive, es decir establece la conexión entre Drive & Colab\r\n","from google.colab import drive\r\n","\r\n","## Con este código se monta el drive en el servidor remoto de Google Colab\r\n","drive.mount('/content/gdrive')\r\n","## Luego se define la ruta completa donde se encuentran almacenados los datos \r\n","ruta = 'gdrive/My Drive/Colab Notebooks/fashion_mnist_data'\r\n","\r\n","X_train, Y_train = load_mnist(ruta, tipo='train')\r\n","X_test, Y_test = load_mnist(ruta, tipo='test')\r\n","\r\n","## Se ejecua esta función, el programa accede a la ruta de drive especificada y por seguridad\r\n","## nos pide un código de autorización para poder acceder a los datos almacenados en nuestra carpeta, ya con esto\r\n","## se llevan los datos desde nuestra carpea a los servidores de google colab. \r\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EkiHSAG9JYJL"},"source":["### Ajusta el tamaño de los datos"]},{"cell_type":"markdown","metadata":{"id":"VCWOl5g4T-kD"},"source":["Antes de crear el modelo se debe hacer dos ajuses en el set de datos:\r\n","\r\n","1.   Se debe garantizar que el set de entrenamiento contenga un numero de registros que sea exactamente un múltiplo de 128, es necesario por que en el caso de la TPU la arquitectura requiere precisamente que durante el entrenamiento presentemos bloques de datos que sean múltiplos de 128, esto no es necesario para la CPU y GPU, pero acá se deja definido de una vez para poder comparar mas adelante los resultados con las otras dos arquitecturas, la siguiente es una forma de calcular el número mas cercano a 60000 y que a su ves es múltiplo de 128:\r\n","\r\n","$$\\frac{60000}{128}*128 = 59904$$\r\n","\r\n","2.   Ajuste de forma equivalente con el set de validación\r\n","\r\n","$$\\frac{10000}{128}*128 = 9984$$\r\n","\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"Nve7cThPH6IS","executionInfo":{"status":"ok","timestamp":1615046054501,"user_tz":300,"elapsed":464,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}}},"source":["## Se reajusta el set de entrenamiento a tamaño 59904 y el set de validación a 9984\r\n","X_train = X_train[0:59904,:]\r\n","X_test = X_test[0:9984,:]\r\n","Y_train = Y_train[0:59904]\r\n","Y_test = Y_test[0:9984]\r\n","## Se utiliza rechape de la libreria Numpy para reajustar el tamaño de los set de datos, y asi garantizar\r\n","## que cada dato es una imagen en escala de grises de 28 x 28 pixeles, que sera la entrada a la red convolucional \r\n","## que queremos entrenar.\r\n","X_train = np.reshape(X_train,(59904,28,28,1))\r\n","X_test = np.reshape(X_test,(9984,28,28,1))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uX00T-1M-sj"},"source":["### Importar TensorFlow 2 (Incluye Keras)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgZyP9sWLlT1","executionInfo":{"status":"ok","timestamp":1615046058212,"user_tz":300,"elapsed":363,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}},"outputId":"28f8bb20-2fb2-4391-d273-544cbcc86c03"},"source":["# Importar la libreria TF 2, en este caso teniendo en cuenta que la plataforma es Google Colab,\r\n","# se debe  especificar que se quiere importar la versión 2 \r\n","%tensorflow_version 2.x   # Para garantizar que la versión 2.x sea importada\r\n","import tensorflow as tf\r\n","print('Versión de TensorFlow: ' + tf.__version__)\r\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: 1.x or 2.x.\n","You set: `2.x   # Para garantizar que la versión 2.x sea importada`. This will be interpreted as: `2.x`.\n","\n","\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n","Versión de TensorFlow: 2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R_JGNxoid2OQ"},"source":["Para implementar el modelo se utiliza la libreria Keras, que viene incorporada en la versión 2 de TF."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMPbF878NdQl","executionInfo":{"status":"ok","timestamp":1615046861712,"user_tz":300,"elapsed":730,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}},"outputId":"3185df92-15a4-4aa2-dada-db625f05ec38"},"source":["\r\n","tf.random.set_seed(200)\r\n","# Se crea el contenedor del modelo utilizando el módulo \"Sequential\", y luego \r\n","# se agregan progresivamente las 3 capas convolucionales, las tres capas son prácticamente\r\n","# identicas.\r\n","model = tf.keras.models.Sequential()\r\n","\r\n","# Primero se hace una normalización de los datos para facilitar la convergencia del\r\n","# entrenamieno, eso se hace con la función \"BatchNormalization\"\r\n","model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\r\n","\r\n","# Se agregan los filtros convolucionales con la función \"Conv2D\", que contiene\r\n","# los siguientes parámetros: \"Conv2D(64, (5, 5), padding='same', activation='elu')\"\r\n","model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\r\n","\r\n","# Luego se agrega una capa \"MaxPooling2D\" que contiene los siguientes parámetros:\r\n","# \"MaxPooling2D(pool_size=(2, 2), strides=(2,2))\"\r\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n","\r\n","# Y finalmente una capa \"Dropout\" para reducir el \"Overfitting\" del modelo\r\n","model.add(tf.keras.layers.Dropout(0.25))\r\n","\r\n","# La única diferencia entre las 3 capas radica en el número de filtros convolucionales\r\n","# utilizados (64, 128 y 256)\r\n","\r\n","model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\r\n","model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\r\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n","model.add(tf.keras.layers.Dropout(0.25))\r\n","\r\n","model.add(tf.keras.layers.BatchNormalization(input_shape=X_train.shape[1:]))\r\n","model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\r\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\r\n","model.add(tf.keras.layers.Dropout(0.25))\r\n","\r\n","# Después de la tercera capa convolucional, se aplanan los datos usando el módulo \"Flatten\"\r\n","# y se agrega una red neuronal con 256 neuronas (\"Dense\") que posteriormene se conecta a la salida\r\n","# la cual contendrá 10 neuronas y una función de activación (\"softmax\") para clasificar cada una de las\r\n","# imagenes que ingresan\r\n","model.add(tf.keras.layers.Flatten())\r\n","model.add(tf.keras.layers.Dense(256))\r\n","model.add(tf.keras.layers.Activation('elu'))\r\n","model.add(tf.keras.layers.Dropout(0.5))\r\n","model.add(tf.keras.layers.Dense(10))\r\n","model.add(tf.keras.layers.Activation('softmax'))\r\n","\r\n","# Se imprimen los resultados del modelo, la implementación en Google Colab es exactamene\r\n","# igual que como si se esuviese ejecutando localmente en nuestro pc\r\n","model.summary()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","batch_normalization_3 (Batch (None, 28, 28, 1)         4         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 28, 28, 64)        1664      \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 14, 14, 128)       204928    \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 7, 7, 256)         819456    \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 3, 3, 256)         0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 3, 3, 256)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2304)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 256)               590080    \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 256)               0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                2570      \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 10)                0         \n","=================================================================\n","Total params: 1,619,470\n","Trainable params: 1,619,084\n","Non-trainable params: 386\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uNy-vqVeSwEt","executionInfo":{"status":"ok","timestamp":1615047109235,"user_tz":300,"elapsed":430,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}}},"source":["# Antes del entrenamiento, se compila el modelo, es decir que se define el optimizador a utilizar (\"adam\"),\r\n","# la función de error (\"crossentropy\") y la métrica de desempeño (\"Precisión\")\r\n","model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TAwafOTOc-U"},"source":["### Entrenamiento con CPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EJtpjqNTObrr","executionInfo":{"status":"ok","timestamp":1614829267690,"user_tz":300,"elapsed":1539725,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}},"outputId":"177195c4-93ba-488a-f515-8cbb48e1de0a"},"source":["# Se procede con el entrenamiento de la red, como es un modelo complejo que cuenta con 1'619.470 parámetros\r\n","# y se esta ejecutando en la arquitectura de CPU, el proceso es demasiado lento, en total tarda aproximadamene 1539 segundos.\r\n","\r\n","# Por este particular solo se realizan 2 iteraciones de entrenamiento (epochs)\r\n","import timeit\r\n","\r\n","# Los parámetros del modelo son:\r\n","# Set de entrenamiento para X, Set de entrenamiento para Y, set´s de validación, numero de registros que contiene\r\n","# el lote de registros y número de iteraciones \"epochs\"\r\n","def entrenamiento_cpu():\r\n","  with tf.device('/cpu:0'):\r\n","    model.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=128,epochs=2,verbose=1)\r\n","  \r\n","  return None\r\n","\r\n","cpu_time = timeit.timeit('entrenamiento_cpu()', number=1, setup='from __main__ import entrenamiento_cpu')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","468/468 [==============================] - 771s 2s/step - loss: 1.0169 - accuracy: 0.7078 - val_loss: 0.4264 - val_accuracy: 0.8551\n","Epoch 2/2\n","468/468 [==============================] - 768s 2s/step - loss: 0.4105 - accuracy: 0.8562 - val_loss: 0.3127 - val_accuracy: 0.8922\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8_0XrvpcTWYT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614829292714,"user_tz":300,"elapsed":415,"user":{"displayName":"Juan Morales","photoUrl":"https://lh3.googleusercontent.com/-5-qHq8-mg-c/AAAAAAAAAAI/AAAAAAAAEdg/nejmrnhRoe8/s64/photo.jpg","userId":"18024484424173226135"}},"outputId":"60665158-8347-48ed-c94f-a3870d625322"},"source":["print('Tiempo de entrenamiento: ' + str(cpu_time) + ' segundos')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tiempo de entrenamiento: 1539.2389175629999 segundos\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u5H_VzRqU4qY"},"source":[""],"execution_count":null,"outputs":[]}]}